{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“Lab3Btorch-student.ipynb”的副本","provenance":[{"file_id":"1LD5CAwpI6fvb7Y5PA2bCRuTileOWiAdf","timestamp":1580388018970},{"file_id":"1cHAdsDRhP6hy3E-RE-ERAr1NrW3JM9DP","timestamp":1580385570788},{"file_id":"1IUoOSHNafurcQ5JSoMZsb8fq9QZycrpe","timestamp":1580384909345},{"file_id":"1_OXlJMYxM9o9AU8LvZAPk36bviv5mQY5","timestamp":1578513429958}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"69z7xsgG0dUE","colab_type":"text"},"source":["# Week 3 lab: Convolutional networks"]},{"cell_type":"markdown","metadata":{"id":"PuYkI8fT0YYH","colab_type":"text"},"source":["This week we will get some hands-on experience with convolutional networks on 2D images."]},{"cell_type":"code","metadata":{"id":"PA84zBjcua7y","colab_type":"code","colab":{}},"source":["#@title\n","from __future__ import division, print_function, unicode_literals\n","from google.colab import files\n","\n","# Common imports\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import seaborn as sns\n","\n","# to make this notebook's output stable across runs\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['axes.labelsize']  = 14\n","plt.rcParams['xtick.labelsize'] = 12\n","plt.rcParams['ytick.labelsize'] = 12\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"tensorflow\"\n","\n","def save_fig(fig_id, tight_layout=True):\n","    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format='png', dpi=300)\n","    files.download(PROJECT_ROOT_DIR+'/images/'+CHAPTER_ID+'/'+fig_id + \".png\") "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YpzHjuUvxgrr","colab_type":"text"},"source":["Now you need to define the network properties. Fill in the missing structure to implement a multi-layer linear, dense network for `model_dense` and a multi-layer convolutional network for `model`. For the convnet, remember that the output of every `Conv2d` and `MaxPool2d` layer is a 3D tensor of shape *(height, width, channels)*. The *width* and *height* dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (e.g. 32 or 64). Remember to check the impact of padding parameters and maxpooling output dimensions when structuring the dimensions of the conv2d layers.\n","\n","The next step would be to feed our last output tensor (of shape (X, Y, N)) into a densely-connected classifier network like those you are already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D, and then add a few Dense layers after that. We are going to do 10-way classification, so use a final layer with 10 outputs and a `nn.Softmax` activation."]},{"cell_type":"code","metadata":{"id":"sQE1avXc0R1Z","colab_type":"code","outputId":"541c1394-7746-40bf-ad25-ff5d87aa1b5c","executionInfo":{"status":"ok","timestamp":1580582752667,"user_tz":0,"elapsed":678,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["import collections\n","\n","model_dense = nn.Sequential(collections.OrderedDict(\n","    [(\"flatten1\", nn.Flatten()),\n","     (\"dense1\", nn.Linear(10, 10)),\n","     (\"dense2\", nn.Linear(10, 10)),\n","     (\"dense3\", nn.Linear(10, 10)),\n","     (\"softmax1\", nn.Softmax(dim=1))]))\n","\n","model = nn.Sequential(collections.OrderedDict(\n","    [(\"conv1\", nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))),\n","     (\"relu1\", nn.ReLU()),\n","     (\"maxpool1\", nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)),\n","     (\"conv2\", nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))),\n","     (\"relu2\", nn.ReLU()),\n","     (\"maxpool2\", nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)),\n","     (\"conv3\", nn.Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1))),\n","     (\"relu3\", nn.ReLU()),\n","     (\"faltten1\", nn.Flatten()),\n","     (\"fc1\", nn.Linear(in_features=144, out_features=64, bias=True)),\n","     (\"relu4\", nn.ReLU()),\n","     (\"fc2\", nn.Linear(in_features=64, out_features=10, bias=True)),\n","     (\"softmax1\", nn.Softmax(dim=1))]))\n","\n","print(model)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu1): ReLU()\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ReLU()\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (relu3): ReLU()\n","  (faltten1): Flatten()\n","  (fc1): Linear(in_features=144, out_features=64, bias=True)\n","  (relu4): ReLU()\n","  (fc2): Linear(in_features=64, out_features=10, bias=True)\n","  (softmax1): Softmax(dim=1)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0eDL77wM2CfU","colab_type":"text"},"source":["Now here's what our network looks like:"]},{"cell_type":"code","metadata":{"id":"hs6msmo32zV3","colab_type":"code","outputId":"21cc8064-1456-4a4e-90bc-3fa4c265d50c","executionInfo":{"status":"ok","timestamp":1580385154310,"user_tz":0,"elapsed":3563,"user":{"displayName":"Roderick Murray-Smith","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCE4aSJyPhkbnhve1ICm0mbxLmLthmBmki6XpUC5g=s64","userId":"15473497901177855861"}},"colab":{"base_uri":"https://localhost:8080/","height":270}},"source":["print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu1): ReLU()\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ReLU()\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (relu3): ReLU()\n","  (Flatten1): Flatten()\n","  (fc1): Linear(in_features=144, out_features=64, bias=True)\n","  (relu4): ReLU()\n","  (fc2): Linear(in_features=64, out_features=10, bias=True)\n","  (softmax1): Softmax(dim=1)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O3SOqIE02Tur","colab_type":"text"},"source":["\n","As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576,), before going through two Dense layers.\n","\n","Now, let's train our convnet on the Fashion MNIST digits. You can[ learn more about the Fashion-MNIST data set](https://github.com/zalandoresearch/fashion-mnist).\n","\n","We specify the root directory to store the dataset, download the training data, if not present on the local machine, and then apply the transforms. ToTensor to turn images into Tensor so we can directly use it with our network. The dataset is stored in the dataset class named `train_set`."]},{"cell_type":"code","metadata":{"id":"ficLeQynybw1","colab_type":"code","outputId":"3c904897-e74b-47e1-f237-205ffd2c8ff4","executionInfo":{"status":"ok","timestamp":1580581927835,"user_tz":0,"elapsed":4741,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["import torchvision\n","import torchvision.transforms as transforms\n","\n","# Use standard FashionMNIST dataset\n","train_set = torchvision.datasets.FashionMNIST(\n","    root = './data/FashionMNIST',\n","    train = True,\n","    download = True,\n","    transform = transforms.Compose([\n","        transforms.ToTensor()                                 \n","    ])\n",")\n","\n","test_set = torchvision.datasets.FashionMNIST(\n","    root = './data/FashionMNIST',\n","    train = False,\n","    download = True,\n","    transform = transforms.Compose([\n","        transforms.ToTensor()                                 \n","    ])\n",")\n","print(train_set, test_set)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:01, 13821291.15it/s]                             \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 95158.31it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["4423680it [00:01, 3900392.08it/s]                             \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 30766.46it/s]            "],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n","Processing...\n","Done!\n","Dataset FashionMNIST\n","    Number of datapoints: 60000\n","    Root location: ./data/FashionMNIST\n","    Split: Train\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","           ) Dataset FashionMNIST\n","    Number of datapoints: 10000\n","    Root location: ./data/FashionMNIST\n","    Split: Test\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","           )\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IGCMMnNBwk3t","colab_type":"code","colab":{}},"source":["import datetime\n","epoch_print_gap = 1\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","        for imgs, labels in train_loader:\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs, labels)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            loss_train += loss.item()\n","            \n","        if epoch == 1 or epoch % epoch_print_gap == 0:\n","            print('{} Epoch {}, Training loss {}'.format(\n","                datetime.datetime.now(), epoch, float(loss_train)))\n","\n","def test_loop(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            #data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmHOauL40nNv","colab_type":"code","colab":{}},"source":["lr = 0.01\n","\n","loader = torch.utils.data.DataLoader(train_set, batch_size = 32)\n","test_loader = torch.utils.data.DataLoader(test_set)\n","#optimizer = optim.Adam(model.parameters(), lr=lr)\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","optimizer_dense = optim.SGD(model_dense.parameters(), lr=lr)\n","\n","images, labels = next(iter(loader))\n","grid = torchvision.utils.make_grid(images)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHM4onO8wr3v","colab_type":"code","outputId":"d1994c65-79c4-4013-e4b3-50ece90896dc","executionInfo":{"status":"error","timestamp":1580583229871,"user_tz":0,"elapsed":458624,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["loss_fn = nn.CrossEntropyLoss()\n","\n","training_loop(\n","    n_epochs = 10, \n","    optimizer = optimizer,\n","    model = model,\n","    loss_fn = loss_fn,\n","    train_loader = loader,\n",")\n","training_loop(                                                      \n","    n_epochs = 10, \n","    optimizer = optimizer_dense,\n","    model = model_dense,\n","    loss_fn = loss_fn,\n","    train_loader = loader,\n",")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["2020-02-01 18:46:57.322560 Epoch 1, Training loss 4316.635025262833\n","2020-02-01 18:47:42.858768 Epoch 2, Training loss 4314.668133497238\n","2020-02-01 18:48:28.131931 Epoch 3, Training loss 4305.92552113533\n","2020-02-01 18:49:14.231814 Epoch 4, Training loss 3898.890065073967\n","2020-02-01 18:49:59.616177 Epoch 5, Training loss 3568.1254167556763\n","2020-02-01 18:50:45.356261 Epoch 6, Training loss 3510.6457827091217\n","2020-02-01 18:51:30.742218 Epoch 7, Training loss 3489.7183204889297\n","2020-02-01 18:52:18.065407 Epoch 8, Training loss 3471.068589091301\n","2020-02-01 18:53:03.908198 Epoch 9, Training loss 3450.9914383888245\n","2020-02-01 18:53:49.410030 Epoch 10, Training loss 3436.8643670082092\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-8d291257ab23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n","\u001b[0;32m<ipython-input-16-1e9b1a3227ae>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 784], m2: [10 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197"]}]},{"cell_type":"markdown","metadata":{"id":"yutyeB-fMkS2","colab_type":"text"},"source":["Now run the test data through to see how the models got on. Compare the perfomance of Linear dense models and conv2D models. Experiment with different numbers and sizes of layers and kernel sizes."]},{"cell_type":"code","metadata":{"id":"Xa7UGRq5E8To","colab_type":"code","colab":{}},"source":["test_loop(model=model, test_loader = test_loader)\n","test_loop(model=model_dense, test_loader = test_loader)"],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab3.ipynb","provenance":[{"file_id":"1tU73Gx9QD7_qcHQc4Mtlua7kriql8Au5","timestamp":1580811152813},{"file_id":"1k64cv9kFW2alLpG0G6XBoNDxALOmAlHj","timestamp":1580755503389},{"file_id":"1rFqp0AL6v8es_a9v0-s26z3Q1C-k-KcU","timestamp":1548695084176},{"file_id":"1uYt5LgcgQCPqnqQq7PK4emEeqWSGB_c6","timestamp":1516297611864},{"file_id":"1uhHEfgLyLaDAyi7bgujs44dUV4ygrpYL","timestamp":1515089002356},{"file_id":"1wsh2_erRCo_w2X2mygJoUa4pBseGJIDq","timestamp":1513774175223},{"file_id":"1hNjXtYEZMlzFe3yJXN-4orZdmfX7QW9B","timestamp":1513773369034},{"file_id":"1Y80cJDxLkDZtvTyRcbPy8xMPAbyNmVXE","timestamp":1513709680152},{"file_id":"1fARiTtWW0pN80eg7XqEhbUbEtelPf5HT","timestamp":1513184264238}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1suGewMzIKsC","colab_type":"text"},"source":["# Lab 3: Language modeling\n","\n","In this week's lab, we'll explore techniques for language modeling. \n","\n","The aims of this lab are:\n","*   Model the probability of generating language with unigrams and trigram LMs\n","*   Evaluate the quality of language model using perplexity\n","*   Understand and address issues of sparsity in language modeling\n","*   Experiment with applications of language models in understanding text"]},{"cell_type":"markdown","metadata":{"id":"napJywoXLO7u","colab_type":"text"},"source":["## Background: Load Reddit\n","Just run the cells below to load the Reddit data. It should be very familiar by now. "]},{"cell_type":"code","metadata":{"id":"OyYm3gDwJKQ-","colab_type":"code","outputId":"fb558687-f5fb-4047-a60f-2115de68e88a","executionInfo":{"status":"ok","timestamp":1581338126434,"user_tz":0,"elapsed":6503,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["local_file = \"coarse_discourse_dump_reddit_unfiltered.json\"\n","!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Copying gs://textasdata/coarse_discourse_dump_reddit.json...\n","- [1 files][ 78.5 MiB/ 78.5 MiB]                                                \n","Operation completed over 1 objects/78.5 MiB.                                     \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mkk-DAI8O1l3","colab_type":"code","outputId":"5441762c-7fa8-4bc4-b4a4-3b8356c4cc2b","executionInfo":{"status":"ok","timestamp":1581338127376,"user_tz":0,"elapsed":7316,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import json\n","import pandas as pd\n","\n","posts = list()\n","\n","# If the dataset is too large, you can load a subset of the posts.\n","post_limit = 100000000\n","\n","# Construct a dataframe, by opening the JSON file line-by-line\n","with open(local_file) as jsonfile:\n","  for i, line in enumerate(jsonfile):\n","    thread = json.loads(line)\n","    if (len(posts) > post_limit):\n","      break\n","      \n","    for post in thread['posts']:\n","      posts.append((thread['subreddit'], thread['title'], thread['url'],\n","                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n","print(len(posts))\n","\n","labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n","post_frame = pd.DataFrame(posts, columns=labels)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["110595\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gB1SEq_B1jYc","colab_type":"code","colab":{}},"source":["#!python -m spacy download en\n","\n","import spacy\n","\n","# Load the small english model. \n","# Disable the advanced NLP features in the pipeline for efficiency.\n","nlp = spacy.load('en_core_web_sm', disable=['ner'])\n","nlp.remove_pipe('tagger')\n","nlp.remove_pipe('parser')\n","\n","#@Tokenize\n","def spacy_tokenize(string):\n","  tokens = list()\n","  doc = nlp(string)\n","  for token in doc:\n","    tokens.append(token)\n","  return tokens\n","\n","#@Normalize\n","def normalize(tokens):\n","  normalized_tokens = list()\n","  for token in tokens:\n","    if (not token.is_punct):\n","#    if (token.is_alpha or token.is_digit):\n","      normalized = token.text.lower().strip()\n","      normalized_tokens.append(normalized)\n","  return normalized_tokens\n","\n","#@Tokenize and normalize\n","def tokenize_normalize(string):\n","  return normalize(spacy_tokenize(string))  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmy9VGvhPsad","colab_type":"text"},"source":["It may take several minutes to run spaCy on all posts in the collection."]},{"cell_type":"code","metadata":{"id":"h_XrKqR6N-8g","colab_type":"code","outputId":"ed129ac6-18c2-4f95-901b-314024f6bf6c","executionInfo":{"status":"ok","timestamp":1581338207760,"user_tz":0,"elapsed":86867,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Use the tokenizer to extract all tokens from the body of the posts.\n","# Flatten the tokens in the post into a single list of all the tokens.\n","import itertools\n","all_tokens = []\n","all_posts_tokenized = post_frame.body.apply(tokenize_normalize)\n","all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n","print(\"Num tokens: \", len(all_tokens))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Num tokens:  4852040\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"78XpfWSHF9n6","colab_type":"text"},"source":["## Unigram language model"]},{"cell_type":"markdown","metadata":{"id":"zrbvxDImHYAd","colab_type":"text"},"source":["We'll use the dictionary from Lab 1 as a building block to build a unigram language model.  For this lab we added special padding tokens (for start /end sentence) that will be used later. "]},{"cell_type":"code","metadata":{"id":"08xONN37bLd7","colab_type":"code","colab":{}},"source":["import collections\n","\n","class SimpleDictionary(object):\n","\n","  # Add special \"padding\" tokens as well as unk.\n","  START_TOKEN = \"<p>\"\n","  END_TOKEN = \"</p>\"\n","  UNK_TOKEN = \"<unk>\"\n","\n","  def __init__(self, tokens, size=None):\n","    self.token_counts = collections.Counter(tokens)\n","    self.N = sum(iter(self.token_counts.values()))\n","\n","    # Leave space for \"<p>\", \"</p>\", and \"<unk>\"\n","    top_counts = self.token_counts.most_common(None if size is None else (size - 3))\n","    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n","             [w for w,c in top_counts])\n","\n","    # Assign an id to each word, by frequency\n","    self.id_to_word = dict(enumerate(vocab))\n","    self.word_to_id = {v:k for k,v in iter(self.id_to_word.items())}\n","    self.size = len(self.id_to_word)\n","    if size is not None:\n","        assert(self.size <= size)\n","\n","    # For convenience create the vocab set\n","    self.vocab_set = set(iter(self.word_to_id.keys()))\n","\n","    # Store special IDs\n","    self.START_ID = self.word_to_id[self.START_TOKEN]\n","    self.END_ID = self.word_to_id[self.END_TOKEN]\n","    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n","  \n","  def words_to_ids(self, words):\n","    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n","\n","  def ids_to_words(self, ids):\n","    return [self.id_to_word[i] for i in ids]\n","\n","  def sentence_to_ids(self, words):\n","    return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n","\n","  def ordered_words(self):\n","    \"\"\"Return a list of words, ordered by id.\"\"\"\n","    return self.ids_to_words(range(self.size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zGf2ADjbWNF","colab_type":"code","outputId":"c8d27e5f-abaf-4011-ee23-a9eb85a77313","executionInfo":{"status":"ok","timestamp":1581338209065,"user_tz":0,"elapsed":85943,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dictionary = SimpleDictionary(all_tokens)\n","print(\"Vocabulary size: %d unique words\" % dictionary.size)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Vocabulary size: 126511 unique words\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2jAqqE58rCTm","colab_type":"text"},"source":["#### Your task\n","\n","- Create a class, ``UnigramLM``\n","- Initializer should take a `dictionary` (``SimpleDictionary``) in its constructor.\n","- It should pre-compute the unigram probabilities for all the words in the `vocab` and store them in a dictionary\n","- For state, keep a copy of the `vocab_set` from the dictionary as a member variable for convenience.\n","- Unknown (OOV) words should return a probability of 0.  Consider using the `defaultdict` collection with a default value of 0.0 (*hint* you may use a lambda).\n","- **Recall from lecture **: What are the two key functions that a language model must satisfy? Define a function for each of these.  \n","  \n","Double click SHOW CODE below for a skeleton (and required function names).  \n"]},{"cell_type":"code","metadata":{"id":"yWYh6lLKYnF2","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","\n","class UnigramLM:\n","\n","  UNK_TOKEN = \"<unk>\"\n","\n","  def __init__(self, dictionary):\n","    self.dictionary = dictionary\n","    self.vocab_set = dictionary.vocab_set\n","\n","    token_counts = dictionary.token_counts\n","    N = dictionary.N\n","    self.vocab_probs = {token: 1.0*count/N for token, count in token_counts.items()}\n","    self.vocab_probs[self.UNK_TOKEN] = 0.0\n","\n","  # Compute the conditional probability of the next token for the unigram model. \n","  def next_token_conditional_prob(self, previous_words, next_word):\n","    if previous_words == None and next_word in self.vocab_set:\n","      return self.vocab_probs[next_word]\n","    if previous_words == None and next_word not in self.vocab_set:\n","      return self.vocab_probs[self.UNK_TOKEN]\n","\n","    condictional_prob = 1.0\n","    for word in previous_words:\n","      condictional_prob *= self.vocab_probs[word]\n","    return condictional_prob * self.vocab_probs[next_word]\n","\n","  # Compute the probability of a sequence, P(w1, ..., wn)\n","  # When Verbose is true, it prints the probability for each\n","  # token in the sequence.\n","  def sequence_probability(self, sequence, verbose=False):\n","    sequence_prob = 1.0\n","    sequence_probs = defaultdict()\n","    for word in sequence:\n","      vocab_prob = self.vocab_probs[word]\n","      sequence_probs[word] = vocab_prob\n","      sequence_prob *= self.vocab_probs[word]\n","    print(sequence_probs)\n","    return sequence_prob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzTWStFirIgk","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","from collections import defaultdict\n","import numpy as np\n","\n","class UnigramLM(object):\n","  \n","  def __init__(self, dictionary):\n","    # Compute the probabilities and store them in a data structure.\n","    self.vocab_set =\n","    \n","  # Compute the conditional probability of the next token for the unigram model. \n","  def next_token_conditional_prob(self, previous_words, next_word):\n","    \n","  # Compute the probability of a sequence, P(w1, ..., wn)\n","  # When Verbose is true, it prints the probability for each\n","  # token in the sequence.\n","  def sequence_probability(self, sequence, verbose=False):"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiiXFQ9ruejG","colab_type":"code","colab":{}},"source":["unigram_lm = UnigramLM(dictionary)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZ1mMO8jxD7W","colab_type":"text"},"source":["Let's try computing the unigram probabilities of words. "]},{"cell_type":"code","metadata":{"id":"MoRWLmmCwa4o","colab_type":"code","outputId":"90c7c5ef-2c7c-44a0-95b3-897de05d6113","executionInfo":{"status":"ok","timestamp":1581338209986,"user_tz":0,"elapsed":880,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# Try computing the unigram probabilities for indivudal words.\n","print(\"Pr(the):\", unigram_lm.next_token_conditional_prob(None, 'the'))\n","print(\"Pr(glasgow):\", unigram_lm.next_token_conditional_prob(None,'glasgow'))\n","print(\"Pr(defenestrate):\", unigram_lm.next_token_conditional_prob(None,'defenestrate'))\n","\n","# And for the same word, but with different contexts\n","print(unigram_lm.next_token_conditional_prob([\"the\", \"end\", \"of\", \"the\", \"world\"], \"as\"))\n","print(unigram_lm.next_token_conditional_prob([\"the\"], \"as\"))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Pr(the): 0.036260212199404784\n","Pr(glasgow): 4.1219775599541635e-07\n","Pr(defenestrate): 0.0\n","2.0035603123200748e-14\n","0.00018948270836924432\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PM6aSVrnzCOX","colab_type":"text"},"source":["The probabilities should be about 3.6% for 'the' and 0 for defenstrate. \n","\n","Now, try computing the probability of a sequence of words.  "]},{"cell_type":"code","metadata":{"id":"bHIVlXHrxb9y","colab_type":"code","outputId":"e47cb107-005f-464f-f362-93e195e80482","executionInfo":{"status":"ok","timestamp":1581338209987,"user_tz":0,"elapsed":868,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["probability = unigram_lm.sequence_probability([\"the\", \"end\", \"of\", \"the\", \"world\", \"as\", \"we\", \"know\", \"it\"], True)\n","print('{:.40f}'.format(probability))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["defaultdict(None, {'the': 0.036260212199404784, 'end': 0.0005247277433821651, 'of': 0.015123123469715831, 'world': 0.00036747429946991367, 'as': 0.005225637051631891, 'we': 0.002661148712706408, 'know': 0.0019861748872639137, 'it': 0.017250682187286173})\n","0.0000000000000000000018268181837699821472\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uFnlo0cSzsLP","colab_type":"text"},"source":["It should be about 1.8×10-21. This is already a very small number and most of the values in these sequence are large by typical probability values. As a result, we usually do the probability computation in log space in practice to avoid problems of underflow."]},{"cell_type":"markdown","metadata":{"id":"5igWwh7t5uLx","colab_type":"text"},"source":["### Application: Spelling correction\n","\n","We can use this simple language model to build a spelling corrector.\n","\n","Below is a spelling corrector based on code by Peter Norvig, a director of research at Google."]},{"cell_type":"markdown","metadata":{"id":"8-PQBbJ_XEh4","colab_type":"text"},"source":["#### Your task\n","Complete the SimpleSpellingCorrector class below. \n","- Fill in the `P` function below to compute the unigram probability of a word.  \n"," - *Hint recall*: What is the conditional probability of a word  in a unigram model? What function function does this correspond to on the `lm` object?\n","-Implement `candidates` to generate a set of the candidate corrections. \n"," - Candidates should use a priority based on edit distance (fewer the better. The words can be up to two edits away. See the provided edit distance functions in the class.  \n"," - Candidates must be in the vocabulary\n"," - *Note*: Even if it is not in the dictionary a word should always be a candidate correction for itself.\n"," - *Hint*: What function in our class checks whether a word or series of words is in the vocabulary?\n"]},{"cell_type":"code","metadata":{"id":"ZjHnVDPsXA_m","colab_type":"code","colab":{}},"source":["class SimpleSpellingCorrector(object): \n","  \n","  def __init__(self, lm):\n","    self.lm = lm\n","  \n","  def P(self, word): \n","    \"Unigram probability of `word`.\"\n","    return self.lm.next_token_conditional_prob(None, word)\n","\n","  def correction(self, word): \n","    \"Most probable spelling correction for word.\"\n","    return max(self.candidates(word), key=self.P)\n","\n","  def candidates(self, word): \n","    \"Generate the union of the sets of spelling correction candidates for a word.\"\n","    candidates = self.edits1(word)\n","    if len(candidates) != 0:\n","      return candidates\n","    else:\n","      return self.edits2(word)\n","\n","  def known(self, words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in self.lm.vocab_set)\n","\n","  def edits1(self, word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters = 'abcdefghijklmnopqrstuvwxyz'\n","    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n","    deletes = [L + R[1:] for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n","    inserts = [L + c + R for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","\n","  def edits2(self, word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vnLb1hI4cyYi","colab_type":"text"},"source":["Now, let's test how the full spelling corrector works. "]},{"cell_type":"code","metadata":{"id":"cRV-SUcO6DJH","colab_type":"code","outputId":"55b28c4e-ae21-46d2-8e9b-d6967dcb1249","executionInfo":{"status":"ok","timestamp":1581338210435,"user_tz":0,"elapsed":1301,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# word = \"speling\"\n","word = \"glashow\" # TRY YOUR WORD!\n","\n","corrector = SimpleSpellingCorrector(unigram_lm)\n","candidates = corrector.candidates(word)\n","print(\"Spelling candidates for:\", word)\n","for candidate in candidates:\n","  print(\"{} \\t{:.9f}\".format(candidate, corrector.P(candidate)))\n","\n","# The correct takes the word that has the highest probability (occurs most often).\n","correction = corrector.correction(word)\n","print(\"\\nSelected correction:\", correction, corrector.P(correction))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Spelling candidates for: glashow\n","zlashow \t0.000000000\n","gilashow \t0.000000000\n","gzlashow \t0.000000000\n","gldshow \t0.000000000\n","glqshow \t0.000000000\n","gashow \t0.000000000\n","glawshow \t0.000000000\n","glashotw \t0.000000000\n","glashoxw \t0.000000000\n","glashdw \t0.000000000\n","glaoshow \t0.000000000\n","xlashow \t0.000000000\n","glavshow \t0.000000000\n","vglashow \t0.000000000\n","glashgow \t0.000000000\n","glashowe \t0.000000000\n","glashowz \t0.000000000\n","gcashow \t0.000000000\n","glashaw \t0.000000000\n","glashofw \t0.000000000\n","glushow \t0.000000000\n","glashsw \t0.000000000\n","glshow \t0.000000000\n","glashowk \t0.000000000\n","olashow \t0.000000000\n","gslashow \t0.000000000\n","aglashow \t0.000000000\n","glkashow \t0.000000000\n","gvlashow \t0.000000000\n","oglashow \t0.000000000\n","gltashow \t0.000000000\n","gzashow \t0.000000000\n","glvashow \t0.000000000\n","glvshow \t0.000000000\n","glasehow \t0.000000000\n","flashow \t0.000000000\n","glashowi \t0.000000000\n","lglashow \t0.000000000\n","glashowg \t0.000000000\n","gelashow \t0.000000000\n","glxshow \t0.000000000\n","glauhow \t0.000000000\n","gloashow \t0.000000000\n","glaahow \t0.000000000\n","glmashow \t0.000000000\n","glashvow \t0.000000000\n","glaschow \t0.000000000\n","glashqw \t0.000000000\n","galshow \t0.000000000\n","gqlashow \t0.000000000\n","glasiow \t0.000000000\n","gflashow \t0.000000000\n","glashob \t0.000000000\n","glashos \t0.000000000\n","glashou \t0.000000000\n","glamhow \t0.000000000\n","glashwo \t0.000000000\n","jlashow \t0.000000000\n","glasohw \t0.000000000\n","glanhow \t0.000000000\n","glashov \t0.000000000\n","glashzw \t0.000000000\n","glatshow \t0.000000000\n","glashmw \t0.000000000\n","glahhow \t0.000000000\n","glasfow \t0.000000000\n","glavhow \t0.000000000\n","glasho \t0.000000000\n","iglashow \t0.000000000\n","glaashow \t0.000000000\n","glasfhow \t0.000000000\n","elashow \t0.000000000\n","dlashow \t0.000000000\n","glashrow \t0.000000000\n","glasthow \t0.000000000\n","galashow \t0.000000000\n","grlashow \t0.000000000\n","glashowd \t0.000000000\n","gxashow \t0.000000000\n","vlashow \t0.000000000\n","glyshow \t0.000000000\n","gladshow \t0.000000000\n","glashobw \t0.000000000\n","glnashow \t0.000000000\n","glasghow \t0.000000000\n","glasmhow \t0.000000000\n","glachow \t0.000000000\n","glasjow \t0.000000000\n","ylashow \t0.000000000\n","glasqow \t0.000000000\n","glasahow \t0.000000000\n","glrashow \t0.000000000\n","gblashow \t0.000000000\n","glashiow \t0.000000000\n","glashon \t0.000000000\n","glbashow \t0.000000000\n","glbshow \t0.000000000\n","glashow \t0.000000000\n","llashow \t0.000000000\n","gljashow \t0.000000000\n","glaphow \t0.000000000\n","glaszow \t0.000000000\n","glashohw \t0.000000000\n","glashnw \t0.000000000\n","glashlow \t0.000000000\n","gkashow \t0.000000000\n","glashxow \t0.000000000\n","ghlashow \t0.000000000\n","glaspow \t0.000000000\n","glashowx \t0.000000000\n","gloshow \t0.000000000\n","gfashow \t0.000000000\n","klashow \t0.000000000\n","glpshow \t0.000000000\n","glzshow \t0.000000000\n","glashtow \t0.000000000\n","glasjhow \t0.000000000\n","glashowa \t0.000000000\n","alashow \t0.000000000\n","gqashow \t0.000000000\n","gllshow \t0.000000000\n","glashowv \t0.000000000\n","ggashow \t0.000000000\n","glashopw \t0.000000000\n","glajhow \t0.000000000\n","glashog \t0.000000000\n","glashw \t0.000000000\n","glashozw \t0.000000000\n","rlashow \t0.000000000\n","glakshow \t0.000000000\n","tlashow \t0.000000000\n","glashowc \t0.000000000\n","glashor \t0.000000000\n","nlashow \t0.000000000\n","glashowp \t0.000000000\n","glashpw \t0.000000000\n","glashowb \t0.000000000\n","glashomw \t0.000000000\n","glaqshow \t0.000000000\n","glashoqw \t0.000000000\n","clashow \t0.000000000\n","glasuow \t0.000000000\n","grashow \t0.000000000\n","glaskow \t0.000000000\n","guashow \t0.000000000\n","giashow \t0.000000000\n","glwshow \t0.000000000\n","gliashow \t0.000000000\n","glayhow \t0.000000000\n","gvashow \t0.000000000\n","glaszhow \t0.000000000\n","glashowj \t0.000000000\n","glashoww \t0.000000000\n","glacshow \t0.000000000\n","glashew \t0.000000000\n","glashcw \t0.000000000\n","glashoyw \t0.000000000\n","glabhow \t0.000000000\n","glmshow \t0.000000000\n","glashok \t0.000000000\n","glcashow \t0.000000000\n","glashowt \t0.000000000\n","slashow \t0.000000000\n","glahsow \t0.000000000\n","glasdhow \t0.000000000\n","gnlashow \t0.000000000\n","gaashow \t0.000000000\n","glashoc \t0.000000000\n","glasuhow \t0.000000000\n","glamshow \t0.000000000\n","glashkow \t0.000000000\n","glasaow \t0.000000000\n","plashow \t0.000000000\n","glashoz \t0.000000000\n","glashfow \t0.000000000\n","glashyw \t0.000000000\n","mlashow \t0.000000000\n","glashoaw \t0.000000000\n","glgshow \t0.000000000\n","glashmow \t0.000000000\n","pglashow \t0.000000000\n","glaghow \t0.000000000\n","glwashow \t0.000000000\n","gldashow \t0.000000000\n","glashxw \t0.000000000\n","glazshow \t0.000000000\n","glashuw \t0.000000000\n","glashqow \t0.000000000\n","glapshow \t0.000000000\n","hglashow \t0.000000000\n","glasnow \t0.000000206\n","xglashow \t0.000000000\n","glastow \t0.000000000\n","goashow \t0.000000000\n","glashnow \t0.000000000\n","glashowl \t0.000000000\n","ilashow \t0.000000000\n","glashop \t0.000000000\n","glazhow \t0.000000000\n","glashhw \t0.000000000\n","glasshow \t0.000000000\n","gklashow \t0.000000000\n","glasheow \t0.000000000\n","cglashow \t0.000000000\n","glashaow \t0.000000000\n","qglashow \t0.000000000\n","uglashow \t0.000000000\n","gltshow \t0.000000000\n","glasbow \t0.000000000\n","gdashow \t0.000000000\n","glcshow \t0.000000000\n","glashhow \t0.000000000\n","glaswow \t0.000000000\n","glashorw \t0.000000000\n","glzashow \t0.000000000\n","glawhow \t0.000000000\n","glashoiw \t0.000000000\n","glasvhow \t0.000000000\n","glaohow \t0.000000000\n","gwlashow \t0.000000000\n","glasholw \t0.000000000\n","glashkw \t0.000000000\n","glaehow \t0.000000000\n","glashoi \t0.000000000\n","glajshow \t0.000000000\n","glashww \t0.000000000\n","glfshow \t0.000000000\n","glashbw \t0.000000000\n","glhshow \t0.000000000\n","glashovw \t0.000000000\n","gyashow \t0.000000000\n","glasohow \t0.000000000\n","glashot \t0.000000000\n","glahshow \t0.000000000\n","glasbhow \t0.000000000\n","glasgow \t0.000000412\n","glasihow \t0.000000000\n","qlashow \t0.000000000\n","glathow \t0.000000000\n","glashows \t0.000000000\n","eglashow \t0.000000000\n","hlashow \t0.000000000\n","gjlashow \t0.000000000\n","lashow \t0.000000000\n","glaihow \t0.000000000\n","gjashow \t0.000000000\n","dglashow \t0.000000000\n","glashuow \t0.000000000\n","glashox \t0.000000000\n","gglashow \t0.000000000\n","glakhow \t0.000000000\n","glashdow \t0.000000000\n","gxlashow \t0.000000000\n","gpashow \t0.000000000\n","glashowr \t0.000000000\n","glashjow \t0.000000000\n","glanshow \t0.000000000\n","glfashow \t0.000000000\n","kglashow \t0.000000000\n","sglashow \t0.000000000\n","gnashow \t0.000000000\n","glabshow \t0.000000000\n","gllashow \t0.000000000\n","glnshow \t0.000000000\n","glashiw \t0.000000000\n","glashom \t0.000000000\n","glasdow \t0.000000000\n","glyashow \t0.000000000\n","gladhow \t0.000000000\n","glashgw \t0.000000000\n","glashojw \t0.000000000\n","gwashow \t0.000000000\n","lgashow \t0.000000000\n","wglashow \t0.000000000\n","glasyhow \t0.000000000\n","glaqhow \t0.000000000\n","glashoew \t0.000000000\n","glashvw \t0.000000000\n","gleashow \t0.000000000\n","glaeshow \t0.000000000\n","ulashow \t0.000000000\n","glasxow \t0.000000000\n","gtlashow \t0.000000000\n","glqashow \t0.000000000\n","glashod \t0.000000000\n","glaxhow \t0.000000000\n","glafhow \t0.000000000\n","glhashow \t0.000000000\n","glashogw \t0.000000000\n","glashzow \t0.000000000\n","glashoq \t0.000000000\n","gljshow \t0.000000000\n","glaslhow \t0.000000000\n","glassow \t0.000000000\n","wlashow \t0.000000000\n","glascow \t0.000000000\n","glalshow \t0.000000000\n","glashokw \t0.000000000\n","glaushow \t0.000000000\n","glxashow \t0.000000000\n","glafshow \t0.000000000\n","glarhow \t0.000000000\n","glashowq \t0.000000000\n","glasnhow \t0.000000000\n","glarshow \t0.000000000\n","fglashow \t0.000000000\n","mglashow \t0.000000000\n","glayshow \t0.000000000\n","glasvow \t0.000000000\n","glasrhow \t0.000000000\n","glashowy \t0.000000000\n","glaseow \t0.000000000\n","gsashow \t0.000000000\n","gmlashow \t0.000000000\n","glashof \t0.000000000\n","glashodw \t0.000000000\n","glashwow \t0.000000000\n","golashow \t0.000000000\n","glashpow \t0.000000000\n","glsashow \t0.000000000\n","gluashow \t0.000000000\n","glasow \t0.000000000\n","glalhow \t0.000000000\n","glaishow \t0.000000000\n","glashoe \t0.000000000\n","jglashow \t0.000000000\n","gmashow \t0.000000000\n","gleshow \t0.000000000\n","glashfw \t0.000000000\n","gbashow \t0.000000000\n","glaswhow \t0.000000000\n","glashoa \t0.000000000\n","glasoow \t0.000000000\n","glsshow \t0.000000000\n","glashocw \t0.000000000\n","glaskhow \t0.000000000\n","glashoo \t0.000000000\n","glashtw \t0.000000000\n","glashjw \t0.000000000\n","glsahow \t0.000000000\n","glashoow \t0.000000000\n","glasmow \t0.000000000\n","glashoy \t0.000000000\n","bglashow \t0.000000000\n","gplashow \t0.000000000\n","gylashow \t0.000000000\n","gdlashow \t0.000000000\n","glishow \t0.000000000\n","glashrw \t0.000000000\n","zglashow \t0.000000000\n","glkshow \t0.000000000\n","blashow \t0.000000000\n","glashol \t0.000000000\n","glasyow \t0.000000000\n","glashowm \t0.000000000\n","glasqhow \t0.000000000\n","glashown \t0.000000000\n","glashyow \t0.000000000\n","glasrow \t0.000000000\n","ghashow \t0.000000000\n","nglashow \t0.000000000\n","glashbow \t0.000000000\n","glashlw \t0.000000000\n","glashonw \t0.000000000\n","gtashow \t0.000000000\n","rglashow \t0.000000000\n","glashouw \t0.000000000\n","glashoh \t0.000000000\n","glashoj \t0.000000000\n","gulashow \t0.000000000\n","glasphow \t0.000000000\n","glaslow \t0.000000000\n","gclashow \t0.000000000\n","glgashow \t0.000000000\n","glrshow \t0.000000000\n","glahow \t0.000000000\n","yglashow \t0.000000000\n","glpashow \t0.000000000\n","glashosw \t0.000000000\n","glagshow \t0.000000000\n","geashow \t0.000000000\n","glashsow \t0.000000000\n","glashowu \t0.000000000\n","glaxshow \t0.000000000\n","glashowf \t0.000000000\n","glashowh \t0.000000000\n","glashcow \t0.000000000\n","glasxhow \t0.000000000\n","glashowo \t0.000000000\n","tglashow \t0.000000000\n","\n","Selected correction: glasgow 4.1219775599541635e-07\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7rrpbrcXecGz","colab_type":"text"},"source":["You should find that you have a reasonable baseline spelling corrector.\n","\n","Spelling candidates for: `speling`\n","\n",">spewing \t0.000001030<br>\n",">spelling \t0.000015251<br>\n",">speeling \t0.000000824<br>\n",">spleling \t0.000000206<br>\n",">Selected correction: spelling 1.5251316971830406e-05\n","\n","Wohooo! We have a very basic reddit spelling corrector. \n","Try it yourself on other words. \n","\n","However, as we'll see it's not perfect.  Let's look at a sequence of words below."]},{"cell_type":"code","metadata":{"id":"V6fMOvSyCJL6","colab_type":"code","outputId":"cd119829-3f60-48ff-d2b9-a1e26fbf5e8a","executionInfo":{"status":"ok","timestamp":1581338210437,"user_tz":0,"elapsed":1289,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Simple function that takes a string as input\n","# and runs the spelling corrector for each token. \n","# Returns a list of spell-corrected tokens.\n","def spell_correct(string, corrector):\n","  corrections = list()\n","  tokens = tokenize_normalize(string)\n","  for t in tokens: \n","    correction = corrector.correction(t)\n","    corrections.append(correction)\n","  return corrections \n","\n","string = \"it is amazon\"\n","print(spell_correct(string, corrector))\n","\n","string = \"http www amazin\"\n","print(spell_correct(string, corrector))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["['i', 'i', 'amazon']\n","['http', 'wow', 'amazing']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OKbC0-w0et6F","colab_type":"text"},"source":["Not so 'amazing'. In both of these cases, we estimated the likelihood of spelling correcting each word on its own,  independently.  This fails in obvious cases where the correct word should be apparent given the sequence. \n","\n","We can do better by taking two factors into account:\n","\n","1.   Computing the probability of the whole sequence of words (in case there are multiple spelling mistakes) \n","2.   Using word context (bi-grams and trigrams) to improve the probablility estimate\n","\n","We'll look at how to compute both in the next section. "]},{"cell_type":"markdown","metadata":{"id":"dRslwUFqDn_t","colab_type":"text"},"source":["# N-Gram Language Models\n","\n","The unigram model isn't a very good one - it doesn't model any previous context. On the other hand, we can't model _all_ of the preceding words, because that history will get prohibitively long and extremely sparse. As a compromise, we make a _Markov assumption_ and limit ourselves to a finite history of $n$ words.\n"]},{"cell_type":"markdown","metadata":{"id":"25Uw9ei-F7ZS","colab_type":"text"},"source":["#### Thought exercise\n","- For a corpus of 1 million tokens and a vocabulary size of 10,000:\n"," - What is the maximum number of bigrams that could theoretically exist? \n"," - How many possible trigrams?\n","\n","Think about this process as drawing a word, then another from the collection to create a sequence. "]},{"cell_type":"code","metadata":{"id":"u4kqEtYAeDsY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"368cba65-5941-4d52-df41-35f59def9ede","executionInfo":{"status":"ok","timestamp":1581338210439,"user_tz":0,"elapsed":1283,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["# length of collection\n","N = 1000000\n","# size of vocabulary\n","V = 10000\n","\n","# Number of possible unique bigrams in the collection\n","bigrams = V * V\n","print(bigrams)\n","\n","# Number of possible unique trigrams in the collection\n","trigrams = V * V * V \n","print(trigrams)\n","\n","# Bigrams and trigrams are limited by the length of the collection. Precisely:\n","# Bigrams would be limited by N-1\n","# Trigrams would be limited by N-2\n","print(min(N-1, bigrams))\n","print(min(N-2, trigrams))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["100000000\n","1000000000000\n","999999\n","999998\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sxibe5r3FVVg","colab_type":"text"},"source":["#### Your task \n","- Print number of possible unique bigrams and trigrams based on the size of the Reddit vocabulary\n","- **Hint**: you stored it in the `dictionary` object. \n"]},{"cell_type":"code","metadata":{"id":"gsA6Dyi6eVIs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4d4e860c-ec09-429d-b270-9f4a87ec5ce5","executionInfo":{"status":"ok","timestamp":1581338210440,"user_tz":0,"elapsed":1278,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["dictionary.size"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["126511"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"aSyqn4e8bmKS","colab_type":"text"},"source":["- How much space would a table of all possible unigrams, bigrams and trigrams take? \n","- For simplicity, let's assume 8 bytes per entry.  \n","- How much space would we need to store all of them? "]},{"cell_type":"code","metadata":{"id":"xEgIEGc-DsEa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"759dae70-6180-462b-96ad-7d0efccfa868","executionInfo":{"status":"ok","timestamp":1581338210442,"user_tz":0,"elapsed":1275,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["import psutil\n","print (\"Vocab size: %d words\" % (dictionary.size))\n","print (\"Unigrams need: %g KB\" % (8 * (dictionary.size ** 1) / (2**10)))\n","print (\"Bigrams need:  %g GB\" % (8 * (dictionary.size ** 2) / (2**30)))\n","print (\"Trigrams need:  %g TB\" % (8 * (dictionary.size ** 3) / (2**40)))\n","print (\"Available:     %g GB\" % (psutil.virtual_memory().available / (2**30)))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Vocab size: 126511 words\n","Unigrams need: 988.367 KB\n","Bigrams need:  119.247 GB\n","Trigrams need:  14732.5 TB\n","Available:     11.223 GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JOoet6sSEbKR","colab_type":"text"},"source":["Look at the results! \n","It should be roughly 120 GB for bigrams. Given a typical machine, we can store all possible unigrams and some of the bigrams, but we'd never get to trigrams!\n","\n","Thankfully, we don't have to store all possible n-gram combinations. Most sequences occur rarely. As a result, for bigrams and trigrams, the table will be very sparse. We only store the entries that we observe; the rest we can taken to be zero or we can estimate their values using smoothing.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8OpxWsOoFf1F","colab_type":"text"},"source":["### Constructing a Trigram Model\n","We'll build a trigram language model, which considers the two preceding words:\n","\n","$$ P(w_i\\ |\\ w_{i-1}, ..., w_0) \\approx P(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n","\n","We'll need to store a table of the word probabilities, indexed by triples $(w_i, w_{i-1}, w_{i-2})$. "]},{"cell_type":"markdown","metadata":{"id":"gC5tanICEigU","colab_type":"text"},"source":["\n","We'll represent our model with a nested map `context => word => probability`, where `word` is $w_i$ and the `context` is the two preceding words $(w_{i-1}, w_{i-2})$ .\n","\n","First, we'll go through the corpus and compute raw trigram counts $count(abc)$, which we'll then normalize with Maximum Likelihood Estimates of the probability:\n","\n","$$  P_{abc} = P(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{\\mathrm{c(abc)}}{\\sum_{c'}\\mathrm{c(abc')}} = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n","\n","*Notational note:* sometimes we write $(w_{i-1}, w_{i-2})$  and sometimes $(w_{i-2}, w_{i-1})$, because P(c|ab) is more readable and natural than P(c|ba).  It doesn't matter as long as the counts are consistent.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HSpzCN8c_OWH","colab_type":"text"},"source":["#### Optional Task \n","Note: This task may take awhile (30 minutes or more) to complete fully.  It is **really optional**, but goes into depth on how to implement an n-gram LM. You can try it yourself, but you should make sure that you understand the working code (provided below). If you are pressed for time don't spend more than 10-15 minutes on this because it is important that you complete the rest of the lab, including evaluation. \n","- Create a class, ``SimpleTrigramLM``\n","- Constructor should take a sequence of tokens and construct the model with n-gram counts\n","- The class should keep the ``vocab`` and its size, like ``SimpleDictionary``\n","- For convenience, keep counts of both the trigram ``counts`` (numerator) and the ``context_totals`` (denominator) values. \n","- The pobability of a sequence should be computed using `log_2` space (use nyumpy) \n","- **Tip:** You might use Python's defaultdict collection (with a lambda) to set default values, particularly for multi-level maps\n","- **Tip:** Use tuples of tokens as keys in the dictionary `(x,y)`, *not* lists of tokens\n","\n","Click SHOW CODE to see the implementation."]},{"cell_type":"code","metadata":{"id":"0d71UAW8_Jzl","colab_type":"code","cellView":"both","colab":{}},"source":["from collections import defaultdict\n","import numpy as np\n","\n","class SimpleTrigramLM(object):\n","  \"\"\"Simple Trigram LM\"\"\"\n","  order_n = 3\n","\n","  def __init__(self, tokens):\n","    \"\"\"Build our trigram model.\n","    Args:\n","      tokens: (list or np.array) of training tokens\n","    Returns:\n","      None\n","    \"\"\"    \n","    # Compute the counts and store them in a data structure.\n","    self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","    self.context_totals = dict()\n","    self.vocab = set()\n","    w_1, w_2 = None, None\n","    for word in tokens:\n","      self.vocab.add(word)\n","      if w_1 is not None and w_2 is not None:\n","        self.counts[(w_2, w_1)][word] += 1\n","      w_2 = w_1\n","      w_1 = word\n","\n","    for context, ctr in iter(self.counts.items()):\n","      self.context_totals[context] = sum(iter(ctr.values()))\n","    \n","    self.V = len(self.vocab)\n","    \n","  # Compute the conditional probability of the next token,  P(wi | wi-1, wi-2)\n","  def next_token_conditional_prob(self, word, seq):\n","    \"\"\"Next token conditional probability.\n","    Args:\n","      word: (string) w in P(w | w_1 w_2 )\n","      seq: (list of string) [w_1, w_2, w_3, ...]\n","    Returns:\n","      (float) P_k(w | w_1 w_2), according to the model\n","    \"\"\"\n","    context = tuple(seq[-2:])\n","    cw = self.counts.get(context, {}).get(word, 0)\n","    cc = self.context_totals.get(context, 0)\n","    return cw / cc\n","\n","    \n","  # Compute the probability of a sequence, P(w1, ..., wn)\n","  def sequence_probability(self, seq, verbose=False):\n","    \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n","    context_size = self.order_n - 1\n","    score = 0.0\n","    count = 0\n","    for i in range(context_size, len(seq)):\n","      context = seq[i-context_size:i]\n","      context_prob = self.next_token_conditional_prob(seq[i], context)\n","      s = np.log2(context_prob)\n","      if verbose:\n","        print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n","      score += s\n","      count += 1\n","       \n","    return score, count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JhKPaDJ_dQz","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","from collections import defaultdict\n","import numpy as np\n","\n","class SimpleTrigramLM(object):\n","    \"\"\"Simple Trigram LM\"\"\"\n","    order_n = 3\n","\n","    def __init__(self, tokens):\n","        \"\"\"Build our trigram model.\n","        Args:\n","          tokens: (list or np.array) of training tokens\n","        Returns:\n","          None\n","        \"\"\"\n","        print(\"Num tokens: \", len(tokens))\n","        \n","        # Raw trigram counts over the corpus.\n","        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n","        # Be sure to use tuples (w_2,w_1) as keys, *not* lists [w_2,w_1]\n","        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","        # Map of (w_1, w_2) -> int\n","        # Entries are c( w_2, w_1 ) = sum_w c(w_2, w_1, w)\n","        self.context_totals = dict()\n","\n","        # Track unique words seen, for normalization\n","        self.vocab = set()\n","\n","        # Iterate through the word stream once\n","        # Compute trigram counts \n","        # This is a sliding window over each word.\n","        w_1, w_2 = None, None\n","        for word in tokens:\n","            self.vocab.add(word)\n","            if w_1 is not None and w_2 is not None:\n","                self.counts[(w_2,w_1)][word] += 1\n","            # Update context\n","            w_2 = w_1\n","            w_1 = word\n","\n","        for context, ctr in iter(self.counts.items()):  \n","            self.context_totals[context] = sum(iter(ctr.values())) \n","\n","        # Total vocabulary size, for normalization\n","        self.V = len(self.vocab)\n","\n","    def next_token_conditional_prob(self, word, seq):\n","        \"\"\"Next token conditional probability.\n","        Args:\n","          word: (string) w in P(w | w_1 w_2 )\n","          seq: (list of string) [w_1, w_2, w_3, ...]\n","        Returns:\n","          (float) P_k(w | w_1 w_2), according to the model\n","        \"\"\"\n","        context = tuple(seq[-2:])  # (w_2, w_1)\n","        cw = self.counts.get(context, {}).get(word, 0)  \n","        numerator = cw \n","        cc = self.context_totals.get(context, 0)  \n","        denominator = cc  \n","        return numerator / denominator  \n","      \n","    def sequence_probability(self, seq, verbose=False):\n","      \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n","      context_size = self.order_n - 1\n","      score = 0.0\n","      count = 0\n","      # Start at third word, since we need a full context.\n","      for i in range(context_size, len(seq)):\n","        context = seq[i-context_size:i]\n","        context_prob = self.next_token_conditional_prob(seq[i], context)\n","        s = np.log2(context_prob)\n","        # DEBUG.\n","        if verbose:\n","            print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n","        score += s\n","        count += 1\n","       \n","      return score, count      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkR9CNqyftiU","colab_type":"text"},"source":["Build the trigram model on all tokens, it should take 15s in colab. \n","\n","- What is it doing during the step?"]},{"cell_type":"code","metadata":{"id":"L5AXZNazHexS","colab_type":"code","outputId":"5aa7d34c-1a2a-4620-e096-f25160438a4e","executionInfo":{"status":"ok","timestamp":1581342021315,"user_tz":0,"elapsed":15448,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import time\n","t0 = time.time()\n","print (\"Building trigram LM...\"),\n","lm = SimpleTrigramLM(all_tokens)\n","print (\"done in %.02f s\" % (time.time() - t0))"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Building trigram LM...\n","Num tokens:  4852040\n","done in 14.91 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dQdWiNkbfxsa","colab_type":"text"},"source":["Let's inspect the trigram output"]},{"cell_type":"code","metadata":{"id":"Hh8gP95nf3SB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":874},"outputId":"2f50cfcb-465a-43d4-e456-ce7701b6238b","executionInfo":{"status":"ok","timestamp":1581342027643,"user_tz":0,"elapsed":6321,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["print (\"Most frequent tri-grams:\")\n","flat_counts = defaultdict(lambda: 0.0)\n","# This code puts the contexts back together with their word counts\n","for (context, counts) in lm.counts.items():\n","  for (word, count) in counts.items():\n","    flat_counts[context, word] = count\n","\n","sorted_trigrams = sorted(flat_counts.items(), key=lambda k_v: k_v[1], reverse=True)\n","\n","for (word, count) in sorted_trigrams[:50]:\n","    print(\"\\\"%s\\\": %d\" % (word, count))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Most frequent tri-grams:\n","\"(('i', 'do'), \"n't\")\": 5739\n","\"(('a', 'lot'), 'of')\": 3015\n","\"(('i', \"'m\"), 'not')\": 2248\n","\"(('you', 'do'), \"n't\")\": 1828\n","\"(('it', \"'s\"), 'a')\": 1826\n","\"(('do', \"n't\"), 'know')\": 1724\n","\"(('', 'i'), \"'m\")\": 1713\n","\"(('it', \"'s\"), 'not')\": 1698\n","\"(('i', 'ca'), \"n't\")\": 1622\n","\"(('if', 'you'), \"'re\")\": 1506\n","\"(('be', 'able'), 'to')\": 1457\n","\"(('', 'if'), 'you')\": 1442\n","\"(('one', 'of'), 'the')\": 1419\n","\"(('do', \"n't\"), 'have')\": 1402\n","\"(('i', \"'ve\"), 'been')\": 1379\n","\"(('i', 'did'), \"n't\")\": 1279\n","\"(('', 'it'), \"'s\")\": 1256\n","\"(('do', \"n't\"), 'think')\": 1220\n","\"(('you', 'want'), 'to')\": 1182\n","\"(('i', 'have'), 'a')\": 1179\n","\"(('if', 'you'), 'have')\": 1102\n","\"(('but', 'it'), \"'s\")\": 1059\n","\"(('if', 'you'), 'want')\": 973\n","\"(('i', 'want'), 'to')\": 917\n","\"(('if', 'you'), 'are')\": 915\n","\"(('you', 'ca'), \"n't\")\": 913\n","\"(('it', 'does'), \"n't\")\": 912\n","\"(('and', 'i'), \"'m\")\": 886\n","\"(('but', 'i'), \"'m\")\": 884\n","\"(('i', 'have'), \"n't\")\": 883\n","\"(('', 'i'), 'do')\": 877\n","\"(('', 'i'), 'have')\": 876\n","\"(('i', 'think'), 'it')\": 866\n","\"(('thanks', 'for'), 'the')\": 863\n","\"(('it', 'would'), 'be')\": 863\n","\"(('if', 'you'), 'do')\": 840\n","\"(('', 'i'), \"'ve\")\": 839\n","\"(('you', 'have'), 'to')\": 839\n","\"(('to', 'be'), 'a')\": 833\n","\"(('going', 'to'), 'be')\": 828\n","\"(('do', \"n't\"), 'want')\": 810\n","\"(('and', 'it'), \"'s\")\": 801\n","\"((\"n't\", 'want'), 'to')\": 791\n","\"(('', 'i'), 'think')\": 785\n","\"(('there', 'is'), 'a')\": 770\n","\"(('you', 'need'), 'to')\": 769\n","\"(('if', 'it'), \"'s\")\": 737\n","\"(('i', \"'m\"), 'a')\": 731\n","\"(('but', 'i'), 'do')\": 710\n","\"(('there', \"'s\"), 'a')\": 710\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i72I2CHUkgMV","colab_type":"text"},"source":["Frequent sequences, are ones like \"a lot of\" (about 3k times). However, given the size of our corpus the counts are still quite small.\n","\n","Let's look at how much memory is used in practice.\n"]},{"cell_type":"code","metadata":{"id":"WZg7-O9skY5N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"8cd723b8-377b-4568-dba5-4939b1b81d87","executionInfo":{"status":"ok","timestamp":1581342027866,"user_tz":0,"elapsed":6527,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["def print_stats(lm):\n","    \"\"\"Output summary statistics about our language model.\"\"\"\n","    print (\"=== N-gram Language Model stats ===\")\n","    unique_ngrams = sum(len(c) for k,c in lm.counts.items())\n","    print (\"%g unique 3-grams\" % (unique_ngrams))\n","\n","    optimal_memory_bytes = sum(\n","            (4 * len(k) + 20 * len(v))\n","             for k, v in lm.counts.items())\n","    print (\"Optimal memory usage (counts only): %d MB\" %\n","            (optimal_memory_bytes / (2**20)))\n","\n","print_stats(lm)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["=== N-gram Language Model stats ===\n","3.15056e+06 unique 3-grams\n","Optimal memory usage (counts only): 70 MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F5OXuFYTkuQa","colab_type":"text"},"source":["Despite the earlier massive theoretical size, the number of unique trigrams in practice is very sparse."]},{"cell_type":"markdown","metadata":{"id":"cZbqWAJFJ_qP","colab_type":"text"},"source":["## Generating made up Reddit posts\n","\n","- Language models are *generative*.  They model the probability of generating a sequence of words. The probabilities can be used in interesting ways, for example to score sequences or even to generate made up text sequences iteratively.  \n","- Below is a function, `sample_next` that samples possible next words randomly proportional to their conditional probability as the next word in the sequence. \n","\n"]},{"cell_type":"code","metadata":{"id":"f-_tAW09B2KH","colab_type":"code","colab":{}},"source":["def sample_next(lm, seq):\n","    \"\"\"Sample a word from the conditional distribution.\"\"\"\n","    # This looks through each possible next word in the vocab and computes its \n","    # conditional probability with the current sequence.\n","    probs = [lm.next_token_conditional_prob(token, seq) for token in lm.vocab]\n","    \n","    # Pick a word at random according to its conditional probability\n","    return np.random.choice(list(lm.vocab), p=probs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjObCY_6lay6","colab_type":"text"},"source":["We can use a Language Model to generate new made up data from our model. We'll generate words sequentially, one token at a time by trying to predict the next word in the sequence."]},{"cell_type":"code","metadata":{"id":"mSI2dfh3e3q3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"71f45594-0e63-4587-8225-b88279ad90d2","executionInfo":{"status":"ok","timestamp":1581342205467,"user_tz":0,"elapsed":49536,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["# Given it the start sequence to indicate the start of a post. \n","   \n","def hallucinate_text(start, max_length):\n","  sequence = list()\n","  sequence.extend(start)\n","  for i in range(max_length - len(start)):\n","      sequence.append(sample_next(lm, sequence))\n","  # print (\" \".join(sequence))\n","  # print (\"[{1:d} tokens; log P(seq): {0:.02f}]\".format(lm.sequence_probability(sequence)))\n","  return sequence\n","    \n","# Maximum length of sequence to generate. \n","max_length = 20\n","\n","# Number of sequences to generate\n","num_sequences = 5\n","\n","\n","start = [\"i\", \"feel\"] # Needs to be an n-gram that occurs in our collection\n","for _ in range(num_sequences):\n","  sequence = hallucinate_text(start, max_length)\n","  print (\" \".join(sequence))\n","    "],"execution_count":77,"outputs":[{"output_type":"stream","text":["i feel that way although if you are a lot of eggs eggwhites various lean meats turkey chicken steak etc\n","i feel as though this is alex 's general i do n't have kids so all humans have not seen\n","i feel has been working applying trp for 4 6 someone mentioned greatshield and bow down to skate by shooting\n","i feel like i commented it post has been a huge sucker for crusty geezers save the world  go\n","i feel its worth it  i hope to be double the winrate is a non traditional but if they\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W4TLERMrKXj7","colab_type":"text"},"source":["We now have a simple bot that can write reddit-like posts!  It sort-of-almost makes sense. Many naive chatbots and spam bots work in a similar way by taking sample text and using it to generate replies.  \n","\n","Have fun with this example by modifying the starting sequence.  Note: currently the starting sequence must occur in our input! You can also modify the length of post to generate or the number of posts.  \n"," - Consider: If you were building an auto-reply system, how could you alter this to take the similarity to an existing starting post? Recall the lab from last lecture.\n","\n","More advanced language models built from neural networks, like BERT are trained on Wikipedia (2.5B words) + BookCorpus (800M words).  We'll learn about these in future lectures.  "]},{"cell_type":"markdown","metadata":{"id":"MVHBk7zEo49K","colab_type":"text"},"source":["Let's evaluate our models by seeing how well they can predict real text."]},{"cell_type":"markdown","metadata":{"id":"buvdn5h1EGnw","colab_type":"text"},"source":["## Language Model Evaluation"]},{"cell_type":"markdown","metadata":{"id":"pIuLPUSIEyJa","colab_type":"text"},"source":["### Train and test sets\n","\n","We'll split the data into two parts: train and test. We train our model (compute counts) on the training sample of posts. This means that we fixed (or *fit* in sci-kit learn terminology) our vocabulary based on the words in *training* set.  We then evaluate on how well it can predict the new unseen test posts.  \n"]},{"cell_type":"code","metadata":{"id":"pcb0HYwKFQ9x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"c5abec8a-8158-4c40-aadc-0f41671933b5","executionInfo":{"status":"ok","timestamp":1581342210610,"user_tz":0,"elapsed":888,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["# We shuffle the posts randomly by calling Pandas sample function with a fraction of 1.\n","shuffled_posts = post_frame.sample(frac=1)\n","\n","# Split the data into 80% train, 20% test posts.\n","train_frac = 0.8\n","split_idx = int(train_frac * len(shuffled_posts))\n","train_posts = shuffled_posts[:split_idx]\n","test_posts = shuffled_posts[split_idx:]\n","\n","print (\"Training set:\", len(train_posts))\n","print (\"Test set: \", len(test_posts))"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Training set: 88476\n","Test set:  22119\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Iz3tO_QZJNJj","colab_type":"text"},"source":["Apply the tokenizer and dictionary to ONLY the training posts."]},{"cell_type":"code","metadata":{"id":"HksEdFwzoKZJ","colab_type":"code","colab":{}},"source":["train_post_tokens = train_posts.body.apply(tokenize_normalize)\n","flat_train_tokens = list(itertools.chain.from_iterable(train_post_tokens))\n","train_flat_dictionary = SimpleDictionary(flat_train_tokens)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOcaQbUdUgzn","colab_type":"text"},"source":["We're about ready to evaluate. But we do some extra text processing steps first. \n","\n","We need to deal with two issues:\n","\n","\n","1.   Add special delimiters to mark the start / end of posts. We want to model the probability of words starting or ending a post. We also want the probability of generating words in a post to be separate (independent) from each other. Note that the number of padding tokens (two at the start) depends on the order of the model (we are using a trigram).\n","2.   Handle UNK (OOV) tokens in new data and replace the words with UNK tokens. \n","\n","The result is a new `post_to_tokens` function.  Although we don't do it here for readability, this function could also convert the token sequence into a sequence of integer word IDs using the dictionary (like we did for a one-hot encoding)."]},{"cell_type":"code","metadata":{"id":"oPnqAGZ7H9i6","colab_type":"code","colab":{}},"source":["# Replace unkown words with the special UNK word.\n","def replace_unk(word, wordset=None):\n","    if word in wordset: \n","      return word \n","    else: \n","      return SimpleDictionary.UNK_TOKEN # unknown token\n","    \n","def posts_to_tokens(posts):\n","    \"\"\"Returns an flattened list of the words in the posts, with padding for a trigram model.\"\"\"\n","    # Pad each post with delimters.\n","    # Why do we use two start delimiters?\n","    padded_posts = ([SimpleDictionary.START_TOKEN, SimpleDictionary.START_TOKEN] + p + [SimpleDictionary.END_TOKEN] for p in posts)\n","    \n","    # This will replace anything not in vocab with <unk> \n","    return np.array([replace_unk(w, wordset=train_flat_dictionary.vocab_set) \n","                     for w in list(itertools.chain.from_iterable((padded_posts)))], dtype=object)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4PHmB1aqXmQ8","colab":{}},"source":["flat_delimited_train_tokens = posts_to_tokens(train_post_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsyXVlBZqEyj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3dd9b832-e6aa-4edb-b23b-a3997f97ff80","executionInfo":{"status":"ok","timestamp":1581342752141,"user_tz":0,"elapsed":18703,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["train_dictionary = SimpleDictionary(flat_delimited_train_tokens)\n","flat_delimited_train_tokens = posts_to_tokens(train_post_tokens)\n","train_lm = SimpleTrigramLM(flat_delimited_train_tokens)\n"],"execution_count":87,"outputs":[{"output_type":"stream","text":["Num tokens:  4152931\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZJIm9npFP8FN","colab_type":"text"},"source":["## Perplexity\n","\n","We'll score our model using perplexity. As mentioned in lecture, we take $ 2^H $ (exponentiate it) to get the perplexity score, where $H$ is the cross-entropy. How do we get this? \n","\n","Running the `sequence_probability` function computes the log-likelihood of our data: \n","\n","$$ Log P(w_1, ... w_N) = \\sum_{i=1}^N \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n","\n","Which is very close to the cross-entropy loss:\n","$$ \\text{H}_{\\text{total}}(y, \\hat{y}) = -1 \\sum_{i=1}^N \\frac{1}{N} \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n","\n","The cross-entropy is equal to $-1$ times the log-likelihood of the data under our model, averaged by the total number of instances (tokens).\n","\n","Let's run it.  We compute the probability of the whole collection as a sequence using our model."]},{"cell_type":"markdown","metadata":{"id":"JlGfGhTyYGHP","colab_type":"text"},"source":["#### Optional Task: \n","- Compute the unigram perplexity of the training data (in log space)\n","- Use the UnigramLM with the train_dictionary\n","- Compare the unigram model perplexity vs the trigram model\n","\n","*Hint:* Compute the probabilities of each word separately since context isn't needed for the unigram model.  Or modify your UnigramLM model to compute sequence probabilities in log space, as we did for the trigram model.\n"]},{"cell_type":"code","metadata":{"id":"lgHQAFL-lRlS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c3324375-8961-4767-fc3c-a27c55ad6ac0","executionInfo":{"status":"ok","timestamp":1581344193795,"user_tz":0,"elapsed":7804,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["log_likelihood = 0.0\n","unigram_train_lm = UnigramLM(train_dictionary)\n","for word in flat_delimited_train_tokens:\n","  log_likelihood += np.log2(unigram_train_lm.next_token_conditional_prob(None, word))\n","log_likelihood = 2**(-log_likelihood/train_dictionary.N)\n","log_likelihood"],"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/plain":["999.2378856294599"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"M7IITLA7UxTL","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a2241fff-98e1-4207-f9ef-0e4aec7320eb","executionInfo":{"status":"ok","timestamp":1581343951848,"user_tz":0,"elapsed":8717,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["#@title\n","log_p = 0 \n","unigram_train_lm = UnigramLM(train_dictionary)\n","# We do this here, we could also modify our LM to operate in log space\n","# directly, which is what is commonly done.\n","for w in flat_delimited_train_tokens:\n","  prob = unigram_train_lm.next_token_conditional_prob(None, w)\n","  if prob == 0: \n","    print (w)\n","  log_prob = np.log2(prob)\n","  log_p += log_prob\n","  \n","print (\"Unigram train perplexity: %.02f\" % (2**(-1*log_p/train_dictionary.N)))"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Unigram train perplexity: 999.24\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QBAvKakIqDNC","colab_type":"text"},"source":["You should find that the the unigram perplexity is slightly less than 1000. This means that on average at each word the model picks between on average 1000 words. "]},{"cell_type":"code","metadata":{"id":"hozSKXzCNBwk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ba3d38cc-dc26-4a84-b8c4-05260ca0b331","executionInfo":{"status":"ok","timestamp":1581344320406,"user_tz":0,"elapsed":23136,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["log_p_data, num_real_tokens = train_lm.sequence_probability(flat_delimited_train_tokens)\n","print (\"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Train perplexity: 10.12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j5fBcXylplNL","colab_type":"text"},"source":["You should see a perplexity of roughly \n","- Train perplexity: 10.13"]},{"cell_type":"markdown","metadata":{"id":"fUTuMCHzO3J3","colab_type":"text"},"source":["The trigram LM reduces the uncertainty of picking the next word by a factor of 100! "]},{"cell_type":"markdown","metadata":{"id":"5fJZkA2y4MFy","colab_type":"text"},"source":["But, this is just on the training data.  How well do the models do on our test data:"]},{"cell_type":"code","metadata":{"id":"16IgxMuwtSnb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"74586d23-12cd-4bc5-f4a2-2e2f0b2d4ecf","executionInfo":{"status":"error","timestamp":1581344420176,"user_tz":0,"elapsed":16751,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["test_post_tokens = test_posts.body.apply(tokenize_normalize)\n","flat_delimited_test_tokens = posts_to_tokens(test_post_tokens)\n","\n","log_p_data, num_real_tokens = train_lm.sequence_probability(flat_delimited_test_tokens)\n","print (\"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))"],"execution_count":108,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log2\n"],"name":"stderr"},{"output_type":"error","ename":"ZeroDivisionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-108-1611a28a3cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mflat_delimited_test_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_post_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlog_p_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_real_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_delimited_test_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Test perplexity: %.02f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog_p_data\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_real_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-619ae1364087>\u001b[0m in \u001b[0;36msequence_probability\u001b[0;34m(self, seq, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mcontext_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_token_conditional_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# DEBUG.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-619ae1364087>\u001b[0m in \u001b[0;36mnext_token_conditional_prob\u001b[0;34m(self, word, seq)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_totals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msequence_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"markdown","metadata":{"id":"XLF043MHQVrT","colab_type":"text"},"source":["Whoops!! What's going on here? Our model gets a low perplexity on the training data, but a divide by zero error (it means a sequence has a zero probability of occurring) on the test data. \n","\n","- Why?\n","\n","**Answer:** the n-gram model overfits without any smoothing; it thinks the generated sequences are completely likely and unseen sequences don't exist. "]},{"cell_type":"markdown","metadata":{"id":"aZTHRMsZQes_","colab_type":"text"},"source":["## Smoothing and handling the unknown\n","\n","Our simple model doesn't have any real mechanism for handling unknown words - if we feed something unseen it will result in an error:"]},{"cell_type":"markdown","metadata":{"id":"SeeAnLtGQtkE","colab_type":"text"},"source":["Is assuming zero probabilities realistic? Let's look back at our unigram distribution:\n","- What percentage of the words in the test tokens are UNK?"]},{"cell_type":"code","metadata":{"id":"hg_wy1eWQxyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c90de9f7-0df5-4ac5-e020-949a5a0fbca4","executionInfo":{"status":"ok","timestamp":1581344474060,"user_tz":0,"elapsed":832,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["print (\"%% <unk> in test set: %.02f%%\" % (np.sum(np.array(flat_delimited_test_tokens) == SimpleDictionary.UNK_TOKEN) * 100.0 / len(flat_delimited_test_tokens)))"],"execution_count":109,"outputs":[{"output_type":"stream","text":["% <unk> in test set: 1.69%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3TsAE_iRZri","colab_type":"text"},"source":["In this case, we find about 1-2% of all tokens are unseen (depending on tokenization). If we want to use our language model in the wild, we'll need to apply smoothing."]},{"cell_type":"markdown","metadata":{"id":"JsmaPLA69UKg","colab_type":"text"},"source":["### Laplace Add-k Smoothing smoothing\n","\n","Remember that if a trigram hasn't been seen before, it will be assigned a zero probability. In this section we'll experiment with Laplace (Add-K) smoothing to fix that problem. Recall our unsmoothed maximum likelihood estimate of $ P(w_i\\ |\\ w_{i-1}, w_{i-2})$ where we use the raw distribution over words seen in a context in the training data:\n","\n","$$  \\hat{P}(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n","\n","Recall from lecture that Add-k smoothing is where we add $k > 0$ to each count $C_{abc}$, pretending we've seen every vocabulary word $k$ extra times in each context. So we have:\n","\n","$$ \\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc} + k}{\\sum_{c'} (C_{abc'} + k)} = \\frac{C_{abc} + k}{C_{ab} + k\\cdot|V|} $$\n","\n","where $|V|$ is the size of our vocabulary."]},{"cell_type":"markdown","metadata":{"id":"l7UVblH05ZE7","colab_type":"text"},"source":["#### Optional exercise\n","NOTE: This exercise is also OPTIONAL.  If you are low on time skip it and examine the working code to be able to experiment with different values of K in the next exercise. \n","\n","- Copy your `SimpleTrigramLm` class and call it `AddKTrigramLM`\n","- Add support for a `k` smoothing member variable with a default value of 0.0\n","- Add a function to `set_k` so that it can be updated\n","- Modify `next_token_conditional_prob` to smooth the probability values with Laplace smoothing.\n"]},{"cell_type":"code","metadata":{"id":"im2sKZVJ56sw","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","import numpy as np\n","\n","class AddKTrigramLM(object):\n","  \"\"\"Add K Trigram LM\"\"\"\n","  order_n = 3\n","\n","  def __init__(self, tokens):\n","    \"\"\"Build our trigram model.\n","    Args:\n","      tokens: (list or np.array) of training tokens\n","    Returns:\n","      None\n","    \"\"\"    \n","    # Compute the counts and store them in a data structure.\n","    self.k = 0.0\n","    self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","    self.context_totals = dict()\n","    self.vocab = set()\n","    w_1, w_2 = None, None\n","    for word in tokens:\n","      self.vocab.add(word)\n","      if w_1 is not None and w_2 is not None:\n","        self.counts[(w_2, w_1)][word] += 1\n","      w_2 = w_1\n","      w_1 = word\n","\n","    for context, ctr in iter(self.counts.items()):\n","      self.context_totals[context] = sum(iter(ctr.values()))\n","    \n","    self.V = len(self.vocab)\n","    \n","  # Compute the conditional probability of the next token,  P(wi | wi-1, wi-2)\n","  def next_token_conditional_prob(self, word, seq):\n","    \"\"\"Next token conditional probability.\n","    Args:\n","      word: (string) w in P(w | w_1 w_2 )\n","      seq: (list of string) [w_1, w_2, w_3, ...]\n","    Returns:\n","      (float) P_k(w | w_1 w_2), according to the model\n","    \"\"\"\n","    context = tuple(seq[-2:])\n","    cw = self.counts.get(context, {}).get(word, 0)\n","    cc = self.context_totals.get(context, 0)\n","    return (cw + self.k) / (cc + self.k*self.V)\n","\n","    \n","  # Compute the probability of a sequence, P(w1, ..., wn)\n","  def sequence_probability(self, seq, verbose=False):\n","    \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n","    context_size = self.order_n - 1\n","    score = 0.0\n","    count = 0\n","    for i in range(context_size, len(seq)):\n","      if (seq[i] == \"<p>\" or seq[i] == \"</p>\"):\n","        continue\n","      context = seq[i-context_size:i]\n","      context_prob = self.next_token_conditional_prob(seq[i], context)\n","      s = np.log2(context_prob)\n","      if verbose:\n","        print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n","      score += s\n","      count += 1\n","       \n","    return score, count\n","  \n","  def set_k(self, k=0.0):\n","    self.k = k"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVKJh5sE9S-Y","colab_type":"code","cellView":"both","colab":{}},"source":["#@title\n","from collections import defaultdict\n","import numpy as np\n","\n","class AddKTrigramLM(object):\n","    \"\"\"Trigram LM with add-k smoothing.\"\"\"\n","    order_n = 3\n","\n","    def __eq__(self, other):\n","        \"\"\"Do not modify.\"\"\"\n","        state_vars = ['k', 'counts', 'context_totals', 'words', 'V']\n","        return all([getattr(self, v) == getattr(other, v) for v in state_vars])\n","\n","    def __init__(self, tokens):\n","        \"\"\"Build our trigram model.\n","        Args:\n","          tokens: (list or np.array) of training tokens\n","        Returns:\n","          None\n","        \"\"\"\n","        print(\"Num tokens: \", len(tokens))\n","        self.k = 0.0\n","        # Raw trigram counts over the corpus.\n","        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n","        # Be sure to use tuples (w_2,w_1) as keys, *not* lists [w_2,w_1]\n","        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","        # Map of (w_1, w_2) -> int\n","        # Entries are c( w_2, w_1 ) = sum_w c(w_2, w_1, w)\n","        self.context_totals = dict()\n","\n","        # Track unique words seen, for normalization\n","        # Use wordset.add(word) to add words\n","        wordset = set()\n","\n","        # Iterate through the word stream once\n","        # Compute trigram counts \n","        # This is a sliding window over each word.\n","        w_1, w_2 = None, None\n","        for word in tokens:\n","            wordset.add(word)\n","            if w_1 is not None and w_2 is not None:\n","                self.counts[(w_2,w_1)][word] += 1\n","            # Update context\n","            w_2 = w_1\n","            w_1 = word\n","\n","        for context, ctr in iter(self.counts.items()):  \n","            self.context_totals[context] = sum(iter(ctr.values())) \n","\n","        # Total vocabulary size, for normalization\n","        self.vocab = wordset\n","        self.V = len(self.vocab)\n","\n","    def set_k(self, k=0.0, **params):\n","        self.k = k\n","\n","    def next_token_conditional_prob(self, word, seq):\n","        \"\"\"Next token conditional probability.\n","        Args:\n","          word: (string) w in P(w | w_1 w_2 )\n","          seq: (list of string) [w_1, w_2, w_3, ...]\n","        Returns:\n","          (float) P_k(w | w_1 w_2), according to the model\n","        \"\"\"\n","        context = tuple(seq[-2:])  # (w_2, w_1)\n","        k = self.k\n","      \n","        cw = self.counts.get(context, {}).get(word, 0)  \n","        numerator = cw + k  # add k smoothing\n","        cc = self.context_totals.get(context, 0)  \n","        denominator = cc + (self.V * k)  \n","        return numerator / denominator  \n","      \n","    def sequence_probability(self, seq, verbose=False):\n","      \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n","      context_size = self.order_n - 1\n","      score = 0.0\n","      count = 0\n","      # Start at third word, since we need a full context.\n","      for i in range(context_size, len(seq)):\n","        if (seq[i] == \"<p>\" or seq[i] == \"</p>\"):\n","            continue  # Don't count special tokens in score.\n","        context = seq[i-context_size:i]\n","        context_prob = self.next_token_conditional_prob(seq[i], context)\n","        s = np.log2(context_prob)\n","        score += s\n","        count += 1\n","        # DEBUG.\n","        if verbose:\n","            print (\"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(context), s))\n","      return score, count      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pV2LF-OA6cc9","colab_type":"code","colab":{}},"source":["smoothed_trigram_lm = AddKTrigramLM(flat_delimited_train_tokens)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5kElIJYSAD6","colab_type":"text"},"source":["#### Exercise\n","- Evaluate the perplexity of the model with different values of K. \n","- What is the best value for k you can find? \n","- What parameter should you use? And why?"]},{"cell_type":"markdown","metadata":{"id":"vO30dnZmS4RF","colab_type":"text"},"source":["Note: Each iteration may take time -- about 10-20 seconds in colab, so be careful how many parameters you test. "]},{"cell_type":"code","metadata":{"id":"21bXD4kwHsjU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"outputId":"c688d23e-0f54-46bb-fa79-e57cacf8c531","executionInfo":{"status":"ok","timestamp":1581345893239,"user_tz":0,"elapsed":122474,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["K = [0.000001, 0.0001, 1.0, 2.0]\n","\n","for k in K:\n","  print(\"Trying k value: \", k)\n","  smoothed_trigram_lm.set_k(k=k)\n","  log_p_data, num_real_tokens = smoothed_trigram_lm.sequence_probability(flat_delimited_train_tokens)\n","  print (\"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))\n","\n","\n","  log_p_data, num_real_tokens = smoothed_trigram_lm.sequence_probability(flat_delimited_test_tokens)\n","  print(\"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens)))\n","  print()"],"execution_count":141,"outputs":[{"output_type":"stream","text":["Trying k value:  1e-06\n","Train perplexity: 10.41\n","Test perplexity: 12250.61\n","\n","Trying k value:  0.0001\n","Train perplexity: 23.74\n","Test perplexity: 3805.16\n","\n","Trying k value:  1.0\n","Train perplexity: 17020.09\n","Test perplexity: 26385.02\n","\n","Trying k value:  2.0\n","Train perplexity: 25595.59\n","Test perplexity: 33421.26\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z0R8WPaZ7ukB","colab_type":"text"},"source":["What happens to the train/test perplexities as the value increases?  Why should we expect this?\n","A curious thing happens with the test perplexity. It starts very high (remember it was infinite without smoothing), it then decreases indicating that the model fits the data better.  Finally, both train and test perplexity increase dramatically as we move away from good parameters.  This is typical of many ML algorithms."]},{"cell_type":"markdown","metadata":{"id":"UtYJitVuJqUo","colab_type":"text"},"source":["## Wrap up\n","\n","Let's go back to our original spelling corrector and see if we can do better with our trigram language model. Recall that the naive spelling corrector made silly mistakes:\n","\n","*   it is amazon -> it is amazon\n","*   http www amazin -> http www amazing\n","\n","\n","\n","If we used our trigram model, could we do better? Let's see how our model would perform by scoring these sequences.\n","**Note: **Change the k value to the best value you found."]},{"cell_type":"code","metadata":{"id":"vWbSotunILxZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"068f6af1-b2d4-46c0-8759-5aa3387514a9","executionInfo":{"status":"ok","timestamp":1581346133404,"user_tz":0,"elapsed":614,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["# [it is amazon]\n","smoothed_trigram_lm.set_k(k=0.0001)\n","print(smoothed_trigram_lm.sequence_probability([\"it\", \"is\", \"amazon\"], verbose=False)[0])\n","\n","# [it is amazing]\n","print(smoothed_trigram_lm.sequence_probability([\"it\", \"is\", \"amazing\"], verbose=False)[0])\n"],"execution_count":158,"outputs":[{"output_type":"stream","text":["-25.29336801371501\n","-9.198280102326015\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iZhHVhccLJ4e","colab_type":"text"},"source":["The unsmoothed probability would be infinite; with our smoothed model the probabilities of the incorrect sequences are infinitessimal compared with the correct ones. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 10 units means one utterance is $2^{10} = 1K$ times more likely!\n","\n","In practice, spelling correctors are more complicated than this simple model. A real spelling corrector must also consider all possible sequences of words and find the most likely sequence! We'll see how this work when we discuss NLP in coming weeks. If you'd like to read more about an application in practice, feel free to read about how shopping service Etsy uses language models in [spelling correction in search](https://codeascraft.com/2017/05/01/modeling-spelling-correction-for-search-at-etsy/).  "]},{"cell_type":"markdown","metadata":{"id":"93XMGHE-dCib","colab_type":"text"},"source":["# End\n","\n","This is the end of Lab 3. Please submit your notebook and complete the lab feedback on Moodle.\n","\n","There is another application of language modelling in the Extra Material below if you would like more fun. Peter Norvig's book \n"]},{"cell_type":"markdown","metadata":{"id":"u0ZpeD5vd_hc","colab_type":"text"},"source":["## Extra material"]},{"cell_type":"markdown","metadata":{"id":"hw6eR5TidzIZ","colab_type":"text"},"source":["### Application 2: Text Segmentation\n","\n","You can see we can apply this **really** simple model to another neat application[1]!\n","\n","In the two cells below, we test out our model by taking a piece of text with all the spaces removed, enumerating all possible ways of splitting it (efficiently using dynamic programming), scoring each, and then returning the highest scoring sequence.\n","\n","[1] Peter Norvig, a director of research at Google implemented this model here:  http://norvig.com/ngrams/ch14.pdf and extends it pretty far.  By the end, he has managed to decrypt WWI encryption using only this simple language model!"]},{"cell_type":"code","metadata":{"id":"MOma5RTxdytu","colab_type":"code","colab":{}},"source":["import math\n","\n","class SimpleSegmenter(object): \n","  \n","  def __init__(self, dict):\n","    self.dict = dict\n","  \n","  def memo(self, fn):\n","    cache = {}\n","    def docache(arg):\n","      if arg in cache:\n","        return cache[arg]\n","      val = fn(arg)\n","      cache[arg] = val\n","      return val\n","    return docache\n","\n","  #@memo\n","  def segment(self, text):\n","    if not text: return []\n","    candidates = ([first]+self.segment(rem) for first, rem in self.splits(text))\n","    return max(candidates, key=lambda w: self.Pwords(w))\n","\n","  def splits(self, text, L=20):\n","    return [(text[:i+1], text[i+1:])\n","            for i in range(min(len(text), L))]\n","\n","  def Pw(self, w):\n","      # We see here that we often perform operations in log space to avoid issues\n","      # of floating point error with very small numbers.\n","      if w in self.dict.vocab_set:\n","          return math.log(self.dict.token_counts[w]) - math.log(self.dict.N)\n","      else:\n","          return math.log(self.dict.N) - 100*len(w)\n","\n","  def Pwords(self, words):\n","    return sum(self.Pw(w) for w in words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhVI4F2YmFLA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c5fdb5ad-ae73-4243-8b55-a7ae13b414af","executionInfo":{"status":"ok","timestamp":1581346536209,"user_tz":0,"elapsed":637,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["segmenter.Pw('hello')"],"execution_count":163,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-9.677882091628957"]},"metadata":{"tags":[]},"execution_count":163}]},{"cell_type":"code","metadata":{"id":"Zsav1I10B_H4","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"788d4629-1cf2-4f0a-fdc7-7e3d474b31ac","executionInfo":{"status":"ok","timestamp":1581346259558,"user_tz":0,"elapsed":2790,"user":{"displayName":"甘梓润","photoUrl":"","userId":"17760055163867737057"}}},"source":["#@title\n","segmenter = SimpleSegmenter(dictionary)\n","print(segmenter.segment('hellotherehowareyou'))\n","print(segmenter.segment('tryyourstringhere'))\n"],"execution_count":160,"outputs":[{"output_type":"stream","text":["['hello', 'there', 'how', 'are', 'you']\n","['try', 'your', 'string', 'here']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZaUBSlT0assn","colab_type":"text"},"source":["We could use this segmenter to optimally tokenize URLs, for example.  This segmenter just uses the simple unigram model, but it could also use more complex LMs.\n","\n","**Question**: What's the smoothing used in this probability?"]}]}